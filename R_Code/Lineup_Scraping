# Load R packages
library(rvest)
library(XML)
library(httr)

# Read in URLs
# Use offset to grab lineup tables on multiple pages
offset <- list(seq(0, 900,  by = 100))

for (i in offset){
  URLs <- paste0("https://www.basketball-reference.com/play-index/lineup_finder.cgi?request=1&match=single&player_id=&lineup_type=5-man&output=per_min&year_id=2000&is_playoffs=N&team_id=&opp_id=&game_num_min=0&game_num_max=99&game_month=&game_location=&game_result=&c1stat=tov&c1comp=ge&c1val=&c2stat=fg_pct&c2comp=ge&c2val=&c3stat=diff_pts&c3comp=ge&c3val=&c4stat=mp&c4comp=ge&c4val=10&order_by=mp&order_by_asc=&offset=", i)
}

# For loop to read in all 10 pages of lineup tables
nba <- list()
for (i in 1:10){
    nb <- html_session(URLs[i]) %>%
    html_node("table#stats") %>%
    html_table()
    
    names(nb) <- col_names
    nb <- nb[-1,]
    
    nb <- nb[!(nb$ranker == "Rk" | nb$ranker == ""),]
    
    #Extract player names
    player_name <- GET(URLs[i]) %>%
      htmlParse() %>%
      xpathSApply("//table//a", xmlGetAttr, "data-tip") %>%
      unlist()
    
    #Extract player references
    player_ref <- GET(URLs[i]) %>%
      htmlParse() %>%
      xpathSApply("//table//a", xmlGetAttr, "href") %>%
      unlist()
    
    player_ref <- grep('^/players/[[:alpha:]]', player_ref, value=TRUE)
    
    # Fill in player references and player names rowrise (10 extra columns - 5 player name columns + 5 player ref columns)
    new_name <- as.data.frame(split(player_name, 1:5))
    new_ref <- as.data.frame(split(player_ref, 1:5))
    
    # Add each page to list of data frames
    nba_lineup <- cbind(nb, new_name, new_ref)
    nba[[i]] <- nba_lineup
    
}

# combine all pages into one data frame 
full_data2000 <- do.call(rbind, nba)

# Save data frame as csv
write.csv(full_data2000, "full_data2000.csv")
